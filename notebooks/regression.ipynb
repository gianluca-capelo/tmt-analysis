{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Import necessary libraries\n",
    "import logging\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, \n",
    "    KFold, \n",
    "    RepeatedKFold, \n",
    "    LeaveOneOut, \n",
    "    StratifiedKFold\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    mean_absolute_error, \n",
    "    r2_score, \n",
    "    explained_variance_score\n",
    ")\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))  \n",
    "\n",
    "def bold_max(df, dataset=\"\", precision=2):\n",
    "    \"\"\"\n",
    "    Return a Styler that bolds the column-wise maxima.\n",
    "\n",
    "    Works with both:\n",
    "    - numeric values\n",
    "    - strings in the format '0.84 ± 0.02'\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with either numeric or 'mean ± std' strings.\n",
    "    dataset : str\n",
    "        A caption or title to display above the table.\n",
    "    precision : int, default 2\n",
    "        Number of decimals to show if numeric.\n",
    "    \"\"\"\n",
    "    def is_string_with_std(val):\n",
    "        return isinstance(val, str) and '±' in val\n",
    "    \n",
    "    def is_string_with_bracket(val):\n",
    "        return isinstance(val, str) and '[' in val\n",
    "\n",
    "    if df.applymap(is_string_with_std).all().all():\n",
    "        # All cells are strings with ±\n",
    "        def highlight_max(col):\n",
    "            means = col.str.extract(r\"(\\d+\\.\\d+) ±\")[0].astype(float)\n",
    "            max_val = means.max()\n",
    "            return ['font-weight: bold' if v == max_val else '' for v in means]\n",
    "\n",
    "        return df.style.set_caption(f\"Dataset: {dataset}\").apply(highlight_max, axis=0)\n",
    "\n",
    "    else:\n",
    "        # Assume numeric DataFrame\n",
    "        return (\n",
    "            df.style\n",
    "              .set_caption(f\"Dataset: {dataset}\")\n",
    "              .format(f\"{{:.{precision}f}}\")\n",
    "              .apply(lambda col: ['font-weight: bold' if v == col.max() else '' for v in col], axis=0)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group\n",
      "1    41\n",
      "0    33\n",
      "Name: count, dtype: int64\n",
      "group\n",
      "1    41\n",
      "0    33\n",
      "Name: count, dtype: int64\n",
      "group\n",
      "1    41\n",
      "0    33\n",
      "Name: count, dtype: int64\n",
      "group\n",
      "1    29\n",
      "0    23\n",
      "Name: count, dtype: int64\n",
      "group\n",
      "1    29\n",
      "0    23\n",
      "Name: count, dtype: int64\n",
      "group\n",
      "1    29\n",
      "0    23\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "processed_path = '../data/processed/'\n",
    "# Read data\n",
    "df_digital_tmt_with_target = pd.read_csv(processed_path + 'df_digital_tmt_with_target.csv') \n",
    "demographic_df = pd.read_csv(processed_path + 'demographic_df.csv') \n",
    "non_digital_df = pd.read_csv(processed_path + 'non_digital_df.csv') \n",
    "df_digital_hand_and_eye = pd.read_csv(processed_path + 'df_digital_hand_and_eye.csv') \n",
    "digital_test_less_subjects = pd.read_csv(processed_path + 'digital_test_less_subjects.csv') \n",
    "non_digital_test_less_subjects = pd.read_csv(processed_path + 'non_digital_test_less_subjects.csv') \n",
    "\n",
    "\n",
    "# Final checks\n",
    "print(df_digital_tmt_with_target['group'].value_counts())\n",
    "print(demographic_df['group'].value_counts())\n",
    "print(non_digital_df['group'].value_counts())\n",
    "print(df_digital_hand_and_eye['group'].value_counts())\n",
    "print(digital_test_less_subjects['group'].value_counts())\n",
    "print(non_digital_test_less_subjects['group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "# Ensure logs folder exists\n",
    "log_dir = \"logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "# Create a fresh log file each run\n",
    "log_filename = os.path.join(log_dir, f\"error_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\")\n",
    "logging.basicConfig(filename=log_filename,\n",
    "                    level=logging.ERROR,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 0. SET-UP GENERAL \n",
    "# ───────────────────────────────────────────────────────────────\n",
    "df_metadata_for_ml = pd.read_csv(\"metadata_for_ml.csv\")\n",
    "df_y = df_metadata_for_ml[['suj', 'mmse']]\n",
    "\n",
    "n_splits = 2\n",
    "n_repeats = 1\n",
    "\n",
    "global_seed = 42\n",
    "inner_cv_seed = 50  # Fixed for reproducibility in inner CV\n",
    "perform_pca = False\n",
    "type_of_csv = 'loo'\n",
    "n_components = 4\n",
    "tune_hyperparameters = False\n",
    "feature_selection = True\n",
    "\n",
    "# datasets = [\n",
    "#             # N=79\n",
    "#             'demographic', \n",
    "#             'digital_test', \n",
    "#             'demographic+digital',\n",
    "#             'non_digital_tests', \n",
    "#             'non_digital_tests+demo',\n",
    "\n",
    "#             # N=56\n",
    "#             'demographic_less_subjects', \n",
    "#             'digital_test_less_subjects', \n",
    "#             'demographic+digital_less',\n",
    "#             'hand_and_eye',\n",
    "#             'hand_and_eye_demo',\n",
    "#             'non_digital_test_less_subjects', \n",
    "#             'non_digital_test_less_subjects+demo'\n",
    "#             ]\n",
    "\n",
    "datasets = ['non_digital_test_less_subjects+demo']\n",
    "\n",
    "\n",
    "def extract_X_y_features_regression(df, metadata):\n",
    "    df = df.merge(metadata, how='inner', on='suj')\n",
    "    df = df.drop(['suj', 'group'], axis=1)\n",
    "    print(\"group in X\", 'group' in df.iloc[:, :-1].columns)\n",
    "    print(\"suj in X\", 'suj' in df.iloc[:, :-1].columns)\n",
    "    display(df.head(2))\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:, -1].values\n",
    "    feature_names = df.columns[:-1]\n",
    "    \n",
    "    return X, y, feature_names \n",
    "\n",
    "def join_and_reorder_regression(df1, df2):\n",
    "    df = df1.join(df2.drop(columns='group'))\n",
    "    cols = [col for col in df.columns if col != 'group'] + ['group']\n",
    "    df = df[cols]\n",
    "    assert df.columns[-1] == 'group', \"'group' is not the last column after reordering\"\n",
    "    return df\n",
    "\n",
    "y_variable = 'mmse'\n",
    "df_y = df_metadata_for_ml[['suj',y_variable]]\n",
    "\n",
    "for value in [True, False]: \n",
    "    perform_pca = value\n",
    "    for dataset in datasets:\n",
    "        try:\n",
    "            print(f\"Starting {dataset}: \\n\\n\")\n",
    "            if perform_pca:\n",
    "                print(\"Performing PCA\")\n",
    "\n",
    "            match dataset:\n",
    "                case 'demographic':\n",
    "                    X, y, feature_names = extract_X_y_features_regression(demographic_df, df_y)\n",
    "\n",
    "                case 'demographic_less_subjects':  \n",
    "                    df = demographic_df.loc[df_digital_hand_and_eye.index]\n",
    "                    X, y, feature_names = extract_X_y_features_regression(df, df_y)\n",
    "\n",
    "                case 'demographic+digital':\n",
    "                    df = join_and_reorder_regression(df_digital_tmt_with_target, demographic_df)\n",
    "                    X, y, feature_names = extract_X_y_features_regression(df, df_y)\n",
    "\n",
    "                case 'demographic+digital_less':\n",
    "                    df = join_and_reorder_regression(df_digital_tmt_with_target.loc[df_digital_hand_and_eye.index], demographic_df)\n",
    "                    X, y, feature_names = extract_X_y_features_regression(df, df_y)\n",
    "\n",
    "                case 'non_digital_tests':\n",
    "                    non_digital_df = non_digital_df.drop(y_variable, axis=1)\n",
    "                    X, y, feature_names = extract_X_y_features_regression(non_digital_df, df_y)\n",
    "\n",
    "                case 'non_digital_tests+demo':\n",
    "                    df = join_and_reorder_regression(non_digital_df.drop(columns=['suj', y_variable]), demographic_df)\n",
    "                    X, y, feature_names = extract_X_y_features_regression(df, df_y)\n",
    "\n",
    "                case 'non_digital_test_less_subjects':\n",
    "                    X, y, feature_names = extract_X_y_features_regression(non_digital_test_less_subjects, df_y)\n",
    "\n",
    "                case 'non_digital_test_less_subjects+demo':\n",
    "                    non_digital_test_less_subjects = non_digital_test_less_subjects.drop(y_variable, axis=1)\n",
    "                    df = join_and_reorder_regression(non_digital_test_less_subjects.drop(columns='suj'), demographic_df)\n",
    "                    X, y, feature_names = extract_X_y_features_regression(df, df_y)\n",
    "\n",
    "                case 'digital_test':\n",
    "                    X, y, feature_names = extract_X_y_features_regression(df_digital_tmt_with_target, df_y)\n",
    "\n",
    "                case 'digital_test_less_subjects':\n",
    "                    X, y, feature_names = extract_X_y_features_regression(digital_test_less_subjects, df_y)\n",
    "\n",
    "                case 'hand_and_eye':\n",
    "                    X, y, feature_names = extract_X_y_features_regression(df_digital_hand_and_eye, df_y)\n",
    "\n",
    "                case 'hand_and_eye_demo':\n",
    "                    df = join_and_reorder_regression(df_digital_hand_and_eye, demographic_df)\n",
    "                    X, y, feature_names = extract_X_y_features_regression(df, df_y)\n",
    "\n",
    "                case _:\n",
    "                    raise ValueError(f'Please select a valid dataset from: {datasets}')\n",
    "\n",
    "\n",
    "            # ───────────────────────────────────────────────────────────────\n",
    "            # 1. DEFINICIÓN DE PARÁMETROS Y MODELOS \n",
    "            # ───────────────────────────────────────────────────────────────\n",
    "\n",
    "            unique, counts = np.unique(y, return_counts=True)\n",
    "            print(\"Class distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "            # Define parameter grids\n",
    "            param_grids = {\n",
    "                \"RandomForestClassifier\": {\n",
    "                    \"classifier__n_estimators\": [100, 500, 700, 1000],\n",
    "                    \"classifier__max_depth\": [None, 10, 20, 30]\n",
    "                },\n",
    "                \"SVC\": {\n",
    "                    \"classifier__C\": [0.1, 1, 10],\n",
    "                    \"classifier__kernel\": ['linear', 'rbf']\n",
    "                },\n",
    "                \"LogisticRegression\": {\n",
    "                    \"classifier__C\": [0.1, 1, 10],\n",
    "                    \"classifier__penalty\": ['l2']\n",
    "                },\n",
    "                \"XGBClassifier\": {\n",
    "                    \"classifier__n_estimators\": [100, 300],\n",
    "                    \"classifier__max_depth\": [3, 5],\n",
    "                    \"classifier__learning_rate\": [0.05, 0.1]\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Define models to evaluate\n",
    "            models = [\n",
    "                # RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "                # SVC(random_state=42, probability=True),\n",
    "                LogisticRegression(max_iter=1000, random_state=42, solver='saga', n_jobs=-1),\n",
    "                # xgb.XGBClassifier(random_state=42, tree_method=\"hist\", eval_metric='logloss',n_jobs=-1)\n",
    "            ]\n",
    "\n",
    "            # ───────────────────────────────────────────────────────────────\n",
    "            # 2. Cross validation\n",
    "            # ───────────────────────────────────────────────────────────────\n",
    "\n",
    "            match type_of_csv:\n",
    "                case 'stratified':\n",
    "                    print(f\"RepeatedStratifiedKFold selected with n_splits = {n_splits} and n_repeats = {n_repeats}\")\n",
    "                    outer_cv = RepeatedStratifiedKFold(\n",
    "                        n_splits=n_splits,\n",
    "                        n_repeats=n_repeats,         \n",
    "                        random_state=global_seed # Global seed\n",
    "                    )\n",
    "                case 'loo':\n",
    "                    print(\"LeaveOneOut selected\")\n",
    "                    outer_cv = LeaveOneOut()\n",
    "                case _:\n",
    "                    print(\"select a valid CV type\")\n",
    "\n",
    "            mean_fpr = np.linspace(0, 1, 100)\n",
    "            all_metrics_df = pd.DataFrame(columns=[\n",
    "                'model', 'repeat', 'fold',   \n",
    "                'accuracy', 'balanced_accuracy', 'precision', \n",
    "                'recall', 'f1', 'auc', 'specificity'\n",
    "            ])\n",
    "\n",
    "            # ───────────────────────────────────────────────────────────────\n",
    "            # 3. External loop \n",
    "            # ───────────────────────────────────────────────────────────────\n",
    "            for model in models:\n",
    "                model_name = model.__class__.__name__\n",
    "                print(f\"\\n🧪 CV for: {model_name}\")\n",
    "\n",
    "                tprs, aucs, best_params_list, fold_metrics = [], [], [], []\n",
    "                feature_importance_counts = {n: 0 for n in feature_names}\n",
    "\n",
    "                # fig, ax = plt.subplots(figsize=(6, 6))\n",
    "                all_y_true, all_y_pred = [], []\n",
    "\n",
    "                # Enumeramos 'repeat' y 'fold' para guardar en métricas\n",
    "                for outer_idx, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):\n",
    "                    n_features = 20\n",
    "                    n_components = 4\n",
    "                    fold = outer_idx  # index of the left-out observation\n",
    "                    print('fold:', fold)\n",
    "\n",
    "                    # ── Split\n",
    "                    X_train, X_test = X[train_idx], X[test_idx]\n",
    "                    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "                    \n",
    "                    # ── Inner CV: estratificado 3-fold con la MISMA semilla por repetición\n",
    "                    inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=inner_cv_seed)\n",
    "                    \n",
    "                    if perform_pca:\n",
    "                        n_components = min(n_components, X_train.shape[1])\n",
    "                        print(\"n_components:\", n_components)\n",
    "                        pca_step = ('pca', PCA(n_components=n_components))\n",
    "                    else:\n",
    "                        pca_step = ('noop', 'passthrough')\n",
    "\n",
    "                    \n",
    "                    if feature_selection:\n",
    "                        n_features = min(n_features, X_train.shape[1])\n",
    "                        print(\"n_features:\", n_features)\n",
    "                        feature_selection_step = ('select', SelectKBest(score_func=f_classif, k=n_features))\n",
    "                    else:\n",
    "                        feature_selection_step = ('noop', 'passthrough')\n",
    "\n",
    "                    pipeline = Pipeline([\n",
    "                        ('imputer', SimpleImputer(strategy='mean')),  # or 'median' depending on your data\n",
    "                        feature_selection_step,\n",
    "                        ('scaler', StandardScaler()),\n",
    "                        pca_step,\n",
    "                        ('classifier', model)\n",
    "                    ])\n",
    "\n",
    "                    # Hiperparámetros\n",
    "                    param_grid = param_grids.get(model_name, {})\n",
    "\n",
    "\n",
    "                    if tune_hyperparameters and param_grid:\n",
    "                        grid = GridSearchCV(\n",
    "                            pipeline,\n",
    "                            param_grid=param_grid,\n",
    "                            cv=inner_cv,               \n",
    "                            scoring='roc_auc',\n",
    "                            n_jobs=-1,\n",
    "                            verbose=0\n",
    "                        )\n",
    "                        grid.fit(X_train, y_train)\n",
    "                        best_model = grid.best_estimator_\n",
    "                        best_params_list.append(grid.best_params_)\n",
    "                    else:\n",
    "                        pipeline.fit(X_train, y_train)\n",
    "                        best_model = pipeline\n",
    "                        best_params_list.append(\"no tuning\")\n",
    "\n",
    "                    # ── Predicción\n",
    "                    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "                    y_pred = best_model.predict(X_test)\n",
    "\n",
    "                    all_y_true.extend(y_test)\n",
    "                    all_y_pred.extend(y_pred)\n",
    "\n",
    "                    fold_metrics.append({\n",
    "                        'model': model_name,\n",
    "                        'fold': fold,              \n",
    "                        'y_test': y_test[0],\n",
    "                        'y_pred': y_pred[0],\n",
    "                        'y_pred_proba': y_pred_proba[0],\n",
    "                        'feature_names': feature_names.values\n",
    "                    })\n",
    "\n",
    "                # ── Guardamos métricas\n",
    "                all_metrics_df = pd.concat([all_metrics_df,\n",
    "                                            pd.DataFrame(fold_metrics)],\n",
    "                                        ignore_index=True)\n",
    "\n",
    "\n",
    "            # save\n",
    "            dir = f'./results/modelling/regression/{datetime.now().strftime(\"%Y-%m-%d\")}'\n",
    "            os.makedirs(dir, exist_ok=True)\n",
    "            if perform_pca:\n",
    "                all_metrics_df.to_csv(f'{dir}/{dataset}_feature_{feature_selection}_n={n_features}_tune={tune_hyperparameters}_LOOCV_PCA_n_components{n_components}_{datetime.now().strftime(\"%s\")[-4:]}.csv',index=False)\n",
    "            else:\n",
    "                all_metrics_df.to_csv(f'{dir}/{dataset}_feature_{feature_selection}_n={n_features}_tune={tune_hyperparameters}_LOOCV_{datetime.now().strftime(\"%s\")[-4:]}.csv',index=False)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            error_msg = traceback.format_exc().strip().split(\"\\n\")[-1]  # only last line of error\n",
    "            logging.error(f\"[{dataset}] PCA={perform_pca} → {error_msg}\")\n",
    "            print(f\"⚠️ An error occurred with dataset {dataset}. Check log file: {log_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "et_feedback",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
