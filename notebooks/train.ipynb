{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T14:38:24.235035Z",
     "start_time": "2025-04-23T14:38:24.219681Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, StratifiedKFold, LeaveOneOut\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    RocCurveDisplay, auc, accuracy_score, precision_score, \n",
    "    recall_score, f1_score, confusion_matrix, classification_report,\n",
    "    balanced_accuracy_score, roc_auc_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import re\n",
    "import scipy\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))  \n",
    "\n",
    "def bold_max(df, dataset=\"\", precision=2):\n",
    "    \"\"\"\n",
    "    Return a Styler that bolds the column-wise maxima.\n",
    "\n",
    "    Works with both:\n",
    "    - numeric values\n",
    "    - strings in the format '0.84 ± 0.02'\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with either numeric or 'mean ± std' strings.\n",
    "    dataset : str\n",
    "        A caption or title to display above the table.\n",
    "    precision : int, default 2\n",
    "        Number of decimals to show if numeric.\n",
    "    \"\"\"\n",
    "    def is_string_with_std(val):\n",
    "        return isinstance(val, str) and '±' in val\n",
    "    \n",
    "    def is_string_with_bracket(val):\n",
    "        return isinstance(val, str) and '[' in val\n",
    "\n",
    "    if df.applymap(is_string_with_std).all().all():\n",
    "        # All cells are strings with ±\n",
    "        def highlight_max(col):\n",
    "            means = col.str.extract(r\"(\\d+\\.\\d+) ±\")[0].astype(float)\n",
    "            max_val = means.max()\n",
    "            return ['font-weight: bold' if v == max_val else '' for v in means]\n",
    "\n",
    "        return df.style.set_caption(f\"Dataset: {dataset}\").apply(highlight_max, axis=0)\n",
    "\n",
    "    else:\n",
    "        # Assume numeric DataFrame\n",
    "        return (\n",
    "            df.style\n",
    "              .set_caption(f\"Dataset: {dataset}\")\n",
    "              .format(f\"{{:.{precision}f}}\")\n",
    "              .apply(lambda col: ['font-weight: bold' if v == col.max() else '' for v in col], axis=0)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d31cc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group\n",
      "1    42\n",
      "0    37\n",
      "Name: count, dtype: int64\n",
      "group\n",
      "1    42\n",
      "0    37\n",
      "Name: count, dtype: int64\n",
      "group\n",
      "1    42\n",
      "0    37\n",
      "Name: count, dtype: int64\n",
      "group\n",
      "1    30\n",
      "0    26\n",
      "Name: count, dtype: int64\n",
      "group\n",
      "1    30\n",
      "0    26\n",
      "Name: count, dtype: int64\n",
      "group\n",
      "1    30\n",
      "0    26\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "processed_path = '../data/processed/'\n",
    "# Read data\n",
    "df_digital_tmt_with_target = pd.read_csv(processed_path + 'df_digital_tmt_with_target.csv') \n",
    "demographic_df = pd.read_csv(processed_path + 'demographic_df.csv') \n",
    "non_digital_df = pd.read_csv(processed_path + 'non_digital_df.csv') \n",
    "df_digital_hand_and_eye = pd.read_csv(processed_path + 'df_digital_hand_and_eye.csv') \n",
    "digital_test_less_subjects = pd.read_csv(processed_path + 'digital_test_less_subjects.csv') \n",
    "non_digital_test_less_subjects = pd.read_csv(processed_path + 'non_digital_test_less_subjects.csv') \n",
    "\n",
    "\n",
    "# Final checks\n",
    "print(df_digital_tmt_with_target['group'].value_counts())\n",
    "print(demographic_df['group'].value_counts())\n",
    "print(non_digital_df['group'].value_counts())\n",
    "print(df_digital_hand_and_eye['group'].value_counts())\n",
    "print(digital_test_less_subjects['group'].value_counts())\n",
    "print(non_digital_test_less_subjects['group'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d5b7c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "74d13a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting non_digital_test_less_subjects: \n",
      "\n",
      "\n",
      "Performing PCA\n",
      "Class distribution: {0: 26, 1: 30}\n",
      "LeaveOneOut selected\n",
      "\n",
      "🧪 CV for: RandomForestClassifier\n",
      "fold: 0\n",
      "n_components: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "n_components: 4\n",
      "fold: 2\n",
      "n_components: 4\n",
      "fold: 3\n",
      "n_components: 4\n",
      "fold: 4\n",
      "n_components: 4\n",
      "fold: 5\n",
      "n_components: 4\n",
      "fold: 6\n",
      "n_components: 4\n",
      "fold: 7\n",
      "n_components: 4\n",
      "fold: 8\n",
      "n_components: 4\n",
      "fold: 9\n",
      "n_components: 4\n",
      "fold: 10\n",
      "n_components: 4\n",
      "fold: 11\n",
      "n_components: 4\n",
      "fold: 12\n",
      "n_components: 4\n",
      "fold: 13\n",
      "n_components: 4\n",
      "fold: 14\n",
      "n_components: 4\n",
      "fold: 15\n",
      "n_components: 4\n",
      "fold: 16\n",
      "n_components: 4\n",
      "fold: 17\n",
      "n_components: 4\n",
      "fold: 18\n",
      "n_components: 4\n",
      "fold: 19\n",
      "n_components: 4\n",
      "fold: 20\n",
      "n_components: 4\n",
      "fold: 21\n",
      "n_components: 4\n",
      "fold: 22\n",
      "n_components: 4\n",
      "fold: 23\n",
      "n_components: 4\n",
      "fold: 24\n",
      "n_components: 4\n",
      "fold: 25\n",
      "n_components: 4\n",
      "fold: 26\n",
      "n_components: 4\n",
      "fold: 27\n",
      "n_components: 4\n",
      "fold: 28\n",
      "n_components: 4\n",
      "fold: 29\n",
      "n_components: 4\n",
      "fold: 30\n",
      "n_components: 4\n",
      "fold: 31\n",
      "n_components: 4\n",
      "fold: 32\n",
      "n_components: 4\n",
      "fold: 33\n",
      "n_components: 4\n",
      "fold: 34\n",
      "n_components: 4\n",
      "fold: 35\n",
      "n_components: 4\n",
      "fold: 36\n",
      "n_components: 4\n",
      "fold: 37\n",
      "n_components: 4\n",
      "fold: 38\n",
      "n_components: 4\n",
      "fold: 39\n",
      "n_components: 4\n",
      "fold: 40\n",
      "n_components: 4\n",
      "fold: 41\n",
      "n_components: 4\n",
      "fold: 42\n",
      "n_components: 4\n",
      "fold: 43\n",
      "n_components: 4\n",
      "fold: 44\n",
      "n_components: 4\n",
      "fold: 45\n",
      "n_components: 4\n",
      "fold: 46\n",
      "n_components: 4\n",
      "fold: 47\n",
      "n_components: 4\n",
      "fold: 48\n",
      "n_components: 4\n",
      "fold: 49\n",
      "n_components: 4\n",
      "fold: 50\n",
      "n_components: 4\n",
      "fold: 51\n",
      "n_components: 4\n",
      "fold: 52\n",
      "n_components: 4\n",
      "fold: 53\n",
      "n_components: 4\n",
      "fold: 54\n",
      "n_components: 4\n",
      "fold: 55\n",
      "n_components: 4\n",
      "\n",
      "🧪 CV for: SVC\n",
      "fold: 0\n",
      "n_components: 4\n",
      "fold: 1\n",
      "n_components: 4\n",
      "fold: 2\n",
      "n_components: 4\n",
      "fold: 3\n",
      "n_components: 4\n",
      "fold: 4\n",
      "n_components: 4\n",
      "fold: 5\n",
      "n_components: 4\n",
      "fold: 6\n",
      "n_components: 4\n",
      "fold: 7\n",
      "n_components: 4\n",
      "fold: 8\n",
      "n_components: 4\n",
      "fold: 9\n",
      "n_components: 4\n",
      "fold: 10\n",
      "n_components: 4\n",
      "fold: 11\n",
      "n_components: 4\n",
      "fold: 12\n",
      "n_components: 4\n",
      "fold: 13\n",
      "n_components: 4\n",
      "fold: 14\n",
      "n_components: 4\n",
      "fold: 15\n",
      "n_components: 4\n",
      "fold: 16\n",
      "n_components: 4\n",
      "fold: 17\n",
      "n_components: 4\n",
      "fold: 18\n",
      "n_components: 4\n",
      "fold: 19\n",
      "n_components: 4\n",
      "fold: 20\n",
      "n_components: 4\n",
      "fold: 21\n",
      "n_components: 4\n",
      "fold: 22\n",
      "n_components: 4\n",
      "fold: 23\n",
      "n_components: 4\n",
      "fold: 24\n",
      "n_components: 4\n",
      "fold: 25\n",
      "n_components: 4\n",
      "fold: 26\n",
      "n_components: 4\n",
      "fold: 27\n",
      "n_components: 4\n",
      "fold: 28\n",
      "n_components: 4\n",
      "fold: 29\n",
      "n_components: 4\n",
      "fold: 30\n",
      "n_components: 4\n",
      "fold: 31\n",
      "n_components: 4\n",
      "fold: 32\n",
      "n_components: 4\n",
      "fold: 33\n",
      "n_components: 4\n",
      "fold: 34\n",
      "n_components: 4\n",
      "fold: 35\n",
      "n_components: 4\n",
      "fold: 36\n",
      "n_components: 4\n",
      "fold: 37\n",
      "n_components: 4\n",
      "fold: 38\n",
      "n_components: 4\n",
      "fold: 39\n",
      "n_components: 4\n",
      "fold: 40\n",
      "n_components: 4\n",
      "fold: 41\n",
      "n_components: 4\n",
      "fold: 42\n",
      "n_components: 4\n",
      "fold: 43\n",
      "n_components: 4\n",
      "fold: 44\n",
      "n_components: 4\n",
      "fold: 45\n",
      "n_components: 4\n",
      "fold: 46\n",
      "n_components: 4\n",
      "fold: 47\n",
      "n_components: 4\n",
      "fold: 48\n",
      "n_components: 4\n",
      "fold: 49\n",
      "n_components: 4\n",
      "fold: 50\n",
      "n_components: 4\n",
      "fold: 51\n",
      "n_components: 4\n",
      "fold: 52\n",
      "n_components: 4\n",
      "fold: 53\n",
      "n_components: 4\n",
      "fold: 54\n",
      "n_components: 4\n",
      "fold: 55\n",
      "n_components: 4\n",
      "\n",
      "🧪 CV for: LogisticRegression\n",
      "fold: 0\n",
      "n_components: 4\n",
      "fold: 1\n",
      "n_components: 4\n",
      "fold: 2\n",
      "n_components: 4\n",
      "fold: 3\n",
      "n_components: 4\n",
      "fold: 4\n",
      "n_components: 4\n",
      "fold: 5\n",
      "n_components: 4\n",
      "fold: 6\n",
      "n_components: 4\n",
      "fold: 7\n",
      "n_components: 4\n",
      "fold: 8\n",
      "n_components: 4\n",
      "fold: 9\n",
      "n_components: 4\n",
      "fold: 10\n",
      "n_components: 4\n",
      "fold: 11\n",
      "n_components: 4\n",
      "fold: 12\n",
      "n_components: 4\n",
      "fold: 13\n",
      "n_components: 4\n",
      "fold: 14\n",
      "n_components: 4\n",
      "fold: 15\n",
      "n_components: 4\n",
      "fold: 16\n",
      "n_components: 4\n",
      "fold: 17\n",
      "n_components: 4\n",
      "fold: 18\n",
      "n_components: 4\n",
      "fold: 19\n",
      "n_components: 4\n",
      "fold: 20\n",
      "n_components: 4\n",
      "fold: 21\n",
      "n_components: 4\n",
      "fold: 22\n",
      "n_components: 4\n",
      "fold: 23\n",
      "n_components: 4\n",
      "fold: 24\n",
      "n_components: 4\n",
      "fold: 25\n",
      "n_components: 4\n",
      "fold: 26\n",
      "n_components: 4\n",
      "fold: 27\n",
      "n_components: 4\n",
      "fold: 28\n",
      "n_components: 4\n",
      "fold: 29\n",
      "n_components: 4\n",
      "fold: 30\n",
      "n_components: 4\n",
      "fold: 31\n",
      "n_components: 4\n",
      "fold: 32\n",
      "n_components: 4\n",
      "fold: 33\n",
      "n_components: 4\n",
      "fold: 34\n",
      "n_components: 4\n",
      "fold: 35\n",
      "n_components: 4\n",
      "fold: 36\n",
      "n_components: 4\n",
      "fold: 37\n",
      "n_components: 4\n",
      "fold: 38\n",
      "n_components: 4\n",
      "fold: 39\n",
      "n_components: 4\n",
      "fold: 40\n",
      "n_components: 4\n",
      "fold: 41\n",
      "n_components: 4\n",
      "fold: 42\n",
      "n_components: 4\n",
      "fold: 43\n",
      "n_components: 4\n",
      "fold: 44\n",
      "n_components: 4\n",
      "fold: 45\n",
      "n_components: 4\n",
      "fold: 46\n",
      "n_components: 4\n",
      "fold: 47\n",
      "n_components: 4\n",
      "fold: 48\n",
      "n_components: 4\n",
      "fold: 49\n",
      "n_components: 4\n",
      "fold: 50\n",
      "n_components: 4\n",
      "fold: 51\n",
      "n_components: 4\n",
      "fold: 52\n",
      "n_components: 4\n",
      "fold: 53\n",
      "n_components: 4\n",
      "fold: 54\n",
      "n_components: 4\n",
      "fold: 55\n",
      "n_components: 4\n",
      "\n",
      "🧪 CV for: XGBClassifier\n",
      "fold: 0\n",
      "n_components: 4\n",
      "fold: 1\n",
      "n_components: 4\n",
      "fold: 2\n",
      "n_components: 4\n",
      "fold: 3\n",
      "n_components: 4\n",
      "fold: 4\n",
      "n_components: 4\n",
      "fold: 5\n",
      "n_components: 4\n",
      "fold: 6\n",
      "n_components: 4\n",
      "fold: 7\n",
      "n_components: 4\n",
      "fold: 8\n",
      "n_components: 4\n",
      "fold: 9\n",
      "n_components: 4\n",
      "fold: 10\n",
      "n_components: 4\n",
      "fold: 11\n",
      "n_components: 4\n",
      "fold: 12\n",
      "n_components: 4\n",
      "fold: 13\n",
      "n_components: 4\n",
      "fold: 14\n",
      "n_components: 4\n",
      "fold: 15\n",
      "n_components: 4\n",
      "fold: 16\n",
      "n_components: 4\n",
      "fold: 17\n",
      "n_components: 4\n",
      "fold: 18\n",
      "n_components: 4\n",
      "fold: 19\n",
      "n_components: 4\n",
      "fold: 20\n",
      "n_components: 4\n",
      "fold: 21\n",
      "n_components: 4\n",
      "fold: 22\n",
      "n_components: 4\n",
      "fold: 23\n",
      "n_components: 4\n",
      "fold: 24\n",
      "n_components: 4\n",
      "fold: 25\n",
      "n_components: 4\n",
      "fold: 26\n",
      "n_components: 4\n",
      "fold: 27\n",
      "n_components: 4\n",
      "fold: 28\n",
      "n_components: 4\n",
      "fold: 29\n",
      "n_components: 4\n",
      "fold: 30\n",
      "n_components: 4\n",
      "fold: 31\n",
      "n_components: 4\n",
      "fold: 32\n",
      "n_components: 4\n",
      "fold: 33\n",
      "n_components: 4\n",
      "fold: 34\n",
      "n_components: 4\n",
      "fold: 35\n",
      "n_components: 4\n",
      "fold: 36\n",
      "n_components: 4\n",
      "fold: 37\n",
      "n_components: 4\n",
      "fold: 38\n",
      "n_components: 4\n",
      "fold: 39\n",
      "n_components: 4\n",
      "fold: 40\n",
      "n_components: 4\n",
      "fold: 41\n",
      "n_components: 4\n",
      "fold: 42\n",
      "n_components: 4\n",
      "fold: 43\n",
      "n_components: 4\n",
      "fold: 44\n",
      "n_components: 4\n",
      "fold: 45\n",
      "n_components: 4\n",
      "fold: 46\n",
      "n_components: 4\n",
      "fold: 47\n",
      "n_components: 4\n",
      "fold: 48\n",
      "n_components: 4\n",
      "fold: 49\n",
      "n_components: 4\n",
      "fold: 50\n",
      "n_components: 4\n",
      "fold: 51\n",
      "n_components: 4\n",
      "fold: 52\n",
      "n_components: 4\n",
      "fold: 53\n",
      "n_components: 4\n",
      "fold: 54\n",
      "n_components: 4\n",
      "fold: 55\n",
      "n_components: 4\n",
      "Starting non_digital_test_less_subjects: \n",
      "\n",
      "\n",
      "Class distribution: {0: 26, 1: 30}\n",
      "LeaveOneOut selected\n",
      "\n",
      "🧪 CV for: RandomForestClassifier\n",
      "fold: 0\n",
      "fold: 1\n",
      "fold: 2\n",
      "fold: 3\n",
      "fold: 4\n",
      "fold: 5\n",
      "fold: 6\n",
      "fold: 7\n",
      "fold: 8\n",
      "fold: 9\n",
      "fold: 10\n",
      "fold: 11\n",
      "fold: 12\n",
      "fold: 13\n",
      "fold: 14\n",
      "fold: 15\n",
      "fold: 16\n",
      "fold: 17\n",
      "fold: 18\n",
      "fold: 19\n",
      "fold: 20\n",
      "fold: 21\n",
      "fold: 22\n",
      "fold: 23\n",
      "fold: 24\n",
      "fold: 25\n",
      "fold: 26\n",
      "fold: 27\n",
      "fold: 28\n",
      "fold: 29\n",
      "fold: 30\n",
      "fold: 31\n",
      "fold: 32\n",
      "fold: 33\n",
      "fold: 34\n",
      "fold: 35\n",
      "fold: 36\n",
      "fold: 37\n",
      "fold: 38\n",
      "fold: 39\n",
      "fold: 40\n",
      "fold: 41\n",
      "fold: 42\n",
      "fold: 43\n",
      "fold: 44\n",
      "fold: 45\n",
      "fold: 46\n",
      "fold: 47\n",
      "fold: 48\n",
      "fold: 49\n",
      "fold: 50\n",
      "fold: 51\n",
      "fold: 52\n",
      "fold: 53\n",
      "fold: 54\n",
      "fold: 55\n",
      "\n",
      "🧪 CV for: SVC\n",
      "fold: 0\n",
      "fold: 1\n",
      "fold: 2\n",
      "fold: 3\n",
      "fold: 4\n",
      "fold: 5\n",
      "fold: 6\n",
      "fold: 7\n",
      "fold: 8\n",
      "fold: 9\n",
      "fold: 10\n",
      "fold: 11\n",
      "fold: 12\n",
      "fold: 13\n",
      "fold: 14\n",
      "fold: 15\n",
      "fold: 16\n",
      "fold: 17\n",
      "fold: 18\n",
      "fold: 19\n",
      "fold: 20\n",
      "fold: 21\n",
      "fold: 22\n",
      "fold: 23\n",
      "fold: 24\n",
      "fold: 25\n",
      "fold: 26\n",
      "fold: 27\n",
      "fold: 28\n",
      "fold: 29\n",
      "fold: 30\n",
      "fold: 31\n",
      "fold: 32\n",
      "fold: 33\n",
      "fold: 34\n",
      "fold: 35\n",
      "fold: 36\n",
      "fold: 37\n",
      "fold: 38\n",
      "fold: 39\n",
      "fold: 40\n",
      "fold: 41\n",
      "fold: 42\n",
      "fold: 43\n",
      "fold: 44\n",
      "fold: 45\n",
      "fold: 46\n",
      "fold: 47\n",
      "fold: 48\n",
      "fold: 49\n",
      "fold: 50\n",
      "fold: 51\n",
      "fold: 52\n",
      "fold: 53\n",
      "fold: 54\n",
      "fold: 55\n",
      "\n",
      "🧪 CV for: LogisticRegression\n",
      "fold: 0\n",
      "fold: 1\n",
      "fold: 2\n",
      "fold: 3\n",
      "fold: 4\n",
      "fold: 5\n",
      "fold: 6\n",
      "fold: 7\n",
      "fold: 8\n",
      "fold: 9\n",
      "fold: 10\n",
      "fold: 11\n",
      "fold: 12\n",
      "fold: 13\n",
      "fold: 14\n",
      "fold: 15\n",
      "fold: 16\n",
      "fold: 17\n",
      "fold: 18\n",
      "fold: 19\n",
      "fold: 20\n",
      "fold: 21\n",
      "fold: 22\n",
      "fold: 23\n",
      "fold: 24\n",
      "fold: 25\n",
      "fold: 26\n",
      "fold: 27\n",
      "fold: 28\n",
      "fold: 29\n",
      "fold: 30\n",
      "fold: 31\n",
      "fold: 32\n",
      "fold: 33\n",
      "fold: 34\n",
      "fold: 35\n",
      "fold: 36\n",
      "fold: 37\n",
      "fold: 38\n",
      "fold: 39\n",
      "fold: 40\n",
      "fold: 41\n",
      "fold: 42\n",
      "fold: 43\n",
      "fold: 44\n",
      "fold: 45\n",
      "fold: 46\n",
      "fold: 47\n",
      "fold: 48\n",
      "fold: 49\n",
      "fold: 50\n",
      "fold: 51\n",
      "fold: 52\n",
      "fold: 53\n",
      "fold: 54\n",
      "fold: 55\n",
      "\n",
      "🧪 CV for: XGBClassifier\n",
      "fold: 0\n",
      "fold: 1\n",
      "fold: 2\n",
      "fold: 3\n",
      "fold: 4\n",
      "fold: 5\n",
      "fold: 6\n",
      "fold: 7\n",
      "fold: 8\n",
      "fold: 9\n",
      "fold: 10\n",
      "fold: 11\n",
      "fold: 12\n",
      "fold: 13\n",
      "fold: 14\n",
      "fold: 15\n",
      "fold: 16\n",
      "fold: 17\n",
      "fold: 18\n",
      "fold: 19\n",
      "fold: 20\n",
      "fold: 21\n",
      "fold: 22\n",
      "fold: 23\n",
      "fold: 24\n",
      "fold: 25\n",
      "fold: 26\n",
      "fold: 27\n",
      "fold: 28\n",
      "fold: 29\n",
      "fold: 30\n",
      "fold: 31\n",
      "fold: 32\n",
      "fold: 33\n",
      "fold: 34\n",
      "fold: 35\n",
      "fold: 36\n",
      "fold: 37\n",
      "fold: 38\n",
      "fold: 39\n",
      "fold: 40\n",
      "fold: 41\n",
      "fold: 42\n",
      "fold: 43\n",
      "fold: 44\n",
      "fold: 45\n",
      "fold: 46\n",
      "fold: 47\n",
      "fold: 48\n",
      "fold: 49\n",
      "fold: 50\n",
      "fold: 51\n",
      "fold: 52\n",
      "fold: 53\n",
      "fold: 54\n",
      "fold: 55\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "# Ensure logs folder exists\n",
    "log_dir = \"logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "# Create a fresh log file each run\n",
    "log_filename = os.path.join(log_dir, f\"error_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\")\n",
    "logging.basicConfig(filename=log_filename,\n",
    "                    level=logging.ERROR,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 0. SET-UP GENERAL \n",
    "# ───────────────────────────────────────────────────────────────\n",
    "n_splits = 2\n",
    "n_repeats = 1\n",
    "\n",
    "global_seed = 42\n",
    "inner_cv_seed = 50  # Fixed for reproducibility in inner CV\n",
    "perform_pca = False\n",
    "type_of_csv = 'loo'\n",
    "n_components = 4\n",
    "\n",
    "datasets = ['demographic', 'demographic+digital','digital_test', \n",
    "            'non_digital_tests', 'non_digital_test_less_subjects', \n",
    "            'digital_test_less_subjects', 'hand_and_eye']\n",
    "\n",
    "datasets = ['non_digital_test_less_subjects']\n",
    "for value in [True, False]:\n",
    "    perform_pca = value\n",
    "    for dataset in datasets:\n",
    "        try:\n",
    "            print(f\"Starting {dataset}: \\n\\n\")\n",
    "            if perform_pca:\n",
    "                print(\"Performing PCA\")\n",
    "\n",
    "            match dataset:\n",
    "                case 'demographic':\n",
    "                    X = demographic_df.iloc[:, :-1].values\n",
    "                    y = demographic_df.iloc[:, -1].values\n",
    "                    feature_names = demographic_df.columns[:-1]\n",
    "                case 'demographic+digital':\n",
    "                    df_digital_plus_demo = df_digital_tmt_with_target.join(demographic_df.drop('group',axis=1))\n",
    "                    df_digital_plus_demo = df_digital_plus_demo[[col for col in df_digital_plus_demo.columns if col != 'group'] + ['group']]\n",
    "                    X = df_digital_plus_demo.iloc[:, :-1].values\n",
    "                    y = df_digital_plus_demo.iloc[:, -1].values\n",
    "                    feature_names = df_digital_plus_demo.columns[:-1]\n",
    "                case 'non_digital_tests':\n",
    "                    X = non_digital_df.iloc[:, :-1].values\n",
    "                    y = non_digital_df.iloc[:, -1].values\n",
    "                    feature_names = non_digital_df.columns[:-1]\n",
    "                case 'non_digital_test_less_subjects':\n",
    "                    X = non_digital_test_less_subjects.iloc[:, :-1].values\n",
    "                    y = non_digital_test_less_subjects.iloc[:, -1].values\n",
    "                    feature_names = non_digital_test_less_subjects.columns[:-1]\n",
    "                case 'digital_test':\n",
    "                    X = df_digital_tmt_with_target.iloc[:, :-1].values\n",
    "                    y = df_digital_tmt_with_target.iloc[:, -1].values\n",
    "                    feature_names = df_digital_tmt_with_target.columns[:-1]\n",
    "                case 'digital_test_less_subjects':\n",
    "                    X = digital_test_less_subjects.iloc[:, :-1].values\n",
    "                    y = digital_test_less_subjects.iloc[:, -1].values\n",
    "                    feature_names = digital_test_less_subjects.columns[:-1]\n",
    "                case 'hand_and_eye':\n",
    "                    X = df_digital_hand_and_eye.iloc[:, :-1].values\n",
    "                    y = df_digital_hand_and_eye.iloc[:, -1].values\n",
    "                    feature_names = df_digital_hand_and_eye.columns[:-1]\n",
    "                case 'hand_and_eye_demo': # hand + eye + demo\n",
    "                    df_hand_eye_plus_demo = df_digital_hand_and_eye.join(demographic_df.drop('group',axis=1))\n",
    "                    df_hand_eye_plus_demo = df_hand_eye_plus_demo[[col for col in df_hand_eye_plus_demo.columns if col != 'group'] + ['group']]\n",
    "                    X = df_hand_eye_plus_demo.iloc[:, :-1].values\n",
    "                    y = df_hand_eye_plus_demo.iloc[:, -1].values\n",
    "                    feature_names = df_hand_eye_plus_demo.columns[:-1]\n",
    "                case _:\n",
    "                    raise ValueError(f'please select a valid dataset from: {datasets}')\n",
    "\n",
    "            # ───────────────────────────────────────────────────────────────\n",
    "            # 1. DEFINICIÓN DE PARÁMETROS Y MODELOS \n",
    "            # ───────────────────────────────────────────────────────────────\n",
    "\n",
    "            unique, counts = np.unique(y, return_counts=True)\n",
    "            print(\"Class distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "            # Define parameter grids\n",
    "            param_grids = {\n",
    "                \"RandomForestClassifier\": {\n",
    "                    \"classifier__n_estimators\": [100, 500, 700, 1000],\n",
    "                    \"classifier__max_depth\": [None, 10, 20, 30]\n",
    "                },\n",
    "                \"SVC\": {\n",
    "                    \"classifier__C\": [0.1, 1, 10],\n",
    "                    \"classifier__kernel\": ['linear', 'rbf']\n",
    "                },\n",
    "                \"LogisticRegression\": {\n",
    "                    \"classifier__C\": [0.1, 1, 10],\n",
    "                    \"classifier__penalty\": ['l2']\n",
    "                },\n",
    "                \"XGBClassifier\": {\n",
    "                    \"classifier__n_estimators\": [100, 300],\n",
    "                    \"classifier__max_depth\": [3, 5],\n",
    "                    \"classifier__learning_rate\": [0.05, 0.1]\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Define models to evaluate\n",
    "            models = [\n",
    "                RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "                SVC(random_state=42, probability=True),\n",
    "                LogisticRegression(max_iter=1000, random_state=42, solver='saga', n_jobs=-1),\n",
    "                xgb.XGBClassifier(random_state=42, tree_method=\"hist\", eval_metric='logloss',n_jobs=-1)\n",
    "            ]\n",
    "\n",
    "            # ───────────────────────────────────────────────────────────────\n",
    "            # 2. Cross validation\n",
    "            # ───────────────────────────────────────────────────────────────\n",
    "\n",
    "            match type_of_csv:\n",
    "                case 'stratified':\n",
    "                    print(f\"RepeatedStratifiedKFold selected with n_splits = {n_splits} and n_repeats = {n_repeats}\")\n",
    "                    outer_cv = RepeatedStratifiedKFold(\n",
    "                        n_splits=n_splits,\n",
    "                        n_repeats=n_repeats,         \n",
    "                        random_state=global_seed # Global seed\n",
    "                    )\n",
    "                case 'loo':\n",
    "                    print(\"LeaveOneOut selected\")\n",
    "                    outer_cv = LeaveOneOut()\n",
    "                case _:\n",
    "                    print(\"select a valid CV type\")\n",
    "\n",
    "            mean_fpr = np.linspace(0, 1, 100)\n",
    "            all_metrics_df = pd.DataFrame(columns=[\n",
    "                'model', 'repeat', 'fold',   \n",
    "                'accuracy', 'balanced_accuracy', 'precision', \n",
    "                'recall', 'f1', 'auc', 'specificity'\n",
    "            ])\n",
    "\n",
    "            # ───────────────────────────────────────────────────────────────\n",
    "            # 3. External loop \n",
    "            # ───────────────────────────────────────────────────────────────\n",
    "            for model in models:\n",
    "                model_name = model.__class__.__name__\n",
    "                print(f\"\\n🧪 CV for: {model_name}\")\n",
    "\n",
    "                tprs, aucs, best_params_list, fold_metrics = [], [], [], []\n",
    "                feature_importance_counts = {n: 0 for n in feature_names}\n",
    "\n",
    "                # fig, ax = plt.subplots(figsize=(6, 6))\n",
    "                all_y_true, all_y_pred = [], []\n",
    "\n",
    "                # Enumeramos 'repeat' y 'fold' para guardar en métricas\n",
    "                for outer_idx, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):\n",
    "                    fold = outer_idx  # index of the left-out observation\n",
    "                    print('fold:', fold)\n",
    "\n",
    "                    # ── Split\n",
    "                    X_train, X_test = X[train_idx], X[test_idx]\n",
    "                    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "                    \n",
    "                    # ── Inner CV: estratificado 3-fold con la MISMA semilla por repetición\n",
    "                    inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=inner_cv_seed)\n",
    "                    \n",
    "                    if perform_pca:\n",
    "                        n_components = min(n_components, X_train.shape[1])\n",
    "                        print(\"n_components:\", n_components)\n",
    "                        pca_step = ('pca', PCA(n_components=n_components))\n",
    "                    else:\n",
    "                        pca_step = ('noop', 'passthrough')\n",
    "\n",
    "                    pipeline = Pipeline([\n",
    "                        ('imputer', SimpleImputer(strategy='mean')),  # or 'median' depending on your data\n",
    "                        ('scaler', StandardScaler()),\n",
    "                        pca_step,\n",
    "                        ('classifier', model)\n",
    "                    ])\n",
    "\n",
    "                    # Hiperparámetros\n",
    "                    param_grid = param_grids.get(model_name, {})\n",
    "\n",
    "                    grid = GridSearchCV(\n",
    "                        pipeline,\n",
    "                        param_grid=param_grid,\n",
    "                        cv=inner_cv,               \n",
    "                        scoring='roc_auc',\n",
    "                        n_jobs=-1,\n",
    "                        verbose=0\n",
    "                    )\n",
    "                    \n",
    "                    grid.fit(X_train, y_train)\n",
    "                    best_model = grid.best_estimator_\n",
    "                    best_params_list.append(grid.best_params_)\n",
    "\n",
    "                    # ── Predicción\n",
    "                    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "                    y_pred = best_model.predict(X_test)\n",
    "\n",
    "                    all_y_true.extend(y_test)\n",
    "                    all_y_pred.extend(y_pred)\n",
    "\n",
    "                    fold_metrics.append({\n",
    "                        'model': model_name,\n",
    "                        'fold': fold,              \n",
    "                        'y_test': y_test[0],\n",
    "                        'y_pred': y_pred[0],\n",
    "                        'y_pred_proba': y_pred_proba[0],\n",
    "                        'feature_names': feature_names.values\n",
    "                    })\n",
    "\n",
    "                # ── Guardamos métricas\n",
    "                all_metrics_df = pd.concat([all_metrics_df,\n",
    "                                            pd.DataFrame(fold_metrics)],\n",
    "                                        ignore_index=True)\n",
    "\n",
    "\n",
    "            # save\n",
    "            dir = f'./results/modelling/{datetime.now().strftime(\"%Y-%m-%d\")}'\n",
    "            os.makedirs(dir, exist_ok=True)\n",
    "            if perform_pca:\n",
    "                all_metrics_df.to_csv(f'{dir}/all_metrics_{dataset}_LOOCV_PCA_n_components{n_components}_{datetime.now().strftime(\"%s\")[-4:]}.csv',index=False)\n",
    "            else:\n",
    "                all_metrics_df.to_csv(f'{dir}/all_metrics_{dataset}_LOOCV_{datetime.now().strftime(\"%s\")[-4:]}.csv',index=False)\n",
    "        except Exception as e:\n",
    "            error_msg = traceback.format_exc().strip().split(\"\\n\")[-1]  # only last line of error\n",
    "            logging.error(f\"[{dataset}] PCA={perform_pca} → {error_msg}\")\n",
    "            print(f\"⚠️ An error occurred with dataset {dataset}. Check log file: {log_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bcc0f760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cosine similarity between LOOCV PCA and full PCA: 0.2499\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Fit PCA on full data\n",
    "pca_full = PCA(n_components=4).fit(X)\n",
    "\n",
    "# Fit PCA on training sets in each LOOCV split and compare\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "X = df_digital_tmt_with_target.iloc[:, :-1].values\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.01)  # Tune threshold if needed\n",
    "X_reduced = selector.fit_transform(X)\n",
    "\n",
    "def remove_highly_correlated(df, threshold=0.95):\n",
    "    corr_matrix = pd.DataFrame(df).corr().abs()\n",
    "    upper = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    return pd.DataFrame(df).drop(columns=to_drop)\n",
    "\n",
    "X_reduced = remove_highly_correlated(X)\n",
    "\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "cos_sims = []\n",
    "\n",
    "for train_idx, test_idx in loo.split(X_reduced):\n",
    "    X_train = X[train_idx]\n",
    "    pca_cv = PCA(n_components=4).fit(X_train)\n",
    "    cos = cosine_similarity(pca_cv.components_, pca_full.components_).mean()\n",
    "    cos_sims.append(cos)\n",
    "\n",
    "print(f\"Average cosine similarity between LOOCV PCA and full PCA: {np.mean(cos_sims):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c0729dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_components = 4\n",
    "X = df_digital_tmt_with_target.iloc[:, :-1].values\n",
    "min(n_components, X.shape[1])\n",
    "n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5de8ad07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_path\n",
    "all_metrics_df2 = pd.read_csv('./results/modelling/2025-06-11/all_metrics_demographic_LOOCV_1749.csv')\n",
    "all_metrics_df2['y_pred'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8726332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_metrics_leave_one_out(df, model_name):\n",
    "    y_test_all = [i[0] for i in df[df['model'] == model_name]['y_test'].values]\n",
    "    y_pred_proba_all = [i[0] for i in df[df['model'] == model_name]['y_pred_proba'].values]\n",
    "    roc_auc_score(y_test_all, y_pred_proba_all)\n",
    "    return pd.DataFrame({\n",
    "            'model': model_name,  \n",
    "            'auc': roc_auc_score(y_test_all, y_pred_proba_all),        \n",
    "            # 'accuracy': accuracy_score(y_test_all, y_pred_proba_all),\n",
    "            # 'balanced_accuracy': balanced_accuracy_score(y_test_all, y_pred_proba_all),\n",
    "            # 'precision': precision_score(y_test_all, y_pred_proba_all),\n",
    "            # 'recall': recall_score(y_test_all, y_pred_proba_all),\n",
    "            # 'f1': f1_score(y_test_all, y_pred_proba_all),\n",
    "            }, index=[0])\n",
    "\n",
    "model_dfs = []\n",
    "for model_name in all_metrics_df['model'].unique():\n",
    "    model_dfs.append(calculate_metrics_leave_one_out(all_metrics_df, model_name))\n",
    "\n",
    "pd.concat(model_dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95fe53b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['digital_test_less_subjects', 'digital_test', 'demographic+digital', 'hand_and_eye', 'digital_test', 'demographic', 'demographic+digital', 'demographic']\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2c7c5eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gus/Documents/REPOS/tmt-analysis/notebooks/results/modelling/2025-06-11/all_metrics_digital_test_less_subjects_LOOCV_PCA_n_components3_1749.csv\n",
      "digital_test_less_subjects\n",
      "/home/gus/Documents/REPOS/tmt-analysis/notebooks/results/modelling/2025-06-11/all_metrics_digital_test_LOOCV_1749.csv\n",
      "digital_test\n",
      "/home/gus/Documents/REPOS/tmt-analysis/notebooks/results/modelling/2025-06-11/all_metrics_demographic+digital_LOOCV_PCA_n_components3_1749.csv\n",
      "demographic+digital\n",
      "/home/gus/Documents/REPOS/tmt-analysis/notebooks/results/modelling/2025-06-11/all_metrics_hand_and_eye_LOOCV_PCA_n_components3_1749.csv\n",
      "hand_and_eye\n",
      "/home/gus/Documents/REPOS/tmt-analysis/notebooks/results/modelling/2025-06-11/all_metrics_digital_test_LOOCV_PCA_n_components3_1749.csv\n",
      "digital_test\n",
      "/home/gus/Documents/REPOS/tmt-analysis/notebooks/results/modelling/2025-06-11/all_metrics_demographic_LOOCV_1749.csv\n",
      "demographic\n",
      "/home/gus/Documents/REPOS/tmt-analysis/notebooks/results/modelling/2025-06-11/all_metrics_demographic+digital_LOOCV_1749.csv\n",
      "demographic+digital\n",
      "/home/gus/Documents/REPOS/tmt-analysis/notebooks/results/modelling/2025-06-11/all_metrics_demographic_LOOCV_PCA_n_components3_1749.csv\n",
      "demographic\n",
      "/home/gus/Documents/REPOS/tmt-analysis/notebooks/results/modelling/2025-06-12/all_metrics_non_digital_test_less_subjects_LOOCV_PCA_n_components4_7995.csv\n",
      "non_digital_test_less_subjects\n",
      "/home/gus/Documents/REPOS/tmt-analysis/notebooks/results/modelling/2025-06-12/all_metrics_non_digital_tests_LOOCV_5806.csv\n",
      "non_digital_tests\n",
      "/home/gus/Documents/REPOS/tmt-analysis/notebooks/results/modelling/2025-06-12/all_metrics_non_digital_test_less_subjects_LOOCV_9261.csv\n",
      "non_digital_test_less_subjects\n",
      "/home/gus/Documents/REPOS/tmt-analysis/notebooks/results/modelling/2025-06-12/all_metrics_digital_test_less_subjects_LOOCV_1749.csv\n",
      "digital_test_less_subjects\n",
      "/home/gus/Documents/REPOS/tmt-analysis/notebooks/results/modelling/2025-06-12/all_metrics_non_digital_tests_LOOCV_PCA_n_components4_4077.csv\n",
      "non_digital_tests\n",
      "/home/gus/Documents/REPOS/tmt-analysis/notebooks/results/modelling/2025-06-12/all_metrics_hand_and_eye_LOOCV_1749.csv\n",
      "hand_and_eye\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics_leave_one_out(df, model_name):\n",
    "    y_true = df[df['model'] == model_name]['y_test'].tolist()\n",
    "    y_pred_proba = df[df['model'] == model_name]['y_pred_proba'].tolist()\n",
    "    # y_pred = [1 if p >= 0.5 else 0 for p in y_pred_proba]  # Binarize\n",
    "    y_pred = df[df['model'] == model_name]['y_pred'].tolist()\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    except ValueError:\n",
    "        auc = np.nan\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'model': [model_name],\n",
    "        'auc': [auc],\n",
    "        'accuracy': [accuracy_score(y_true, y_pred)],\n",
    "        'balanced_accuracy': [balanced_accuracy_score(y_true, y_pred)],\n",
    "        'precision': [precision_score(y_true, y_pred, zero_division=0)],\n",
    "        'recall': [recall_score(y_true, y_pred, zero_division=0)],\n",
    "        'f1': [f1_score(y_true, y_pred, zero_division=0)]\n",
    "    })\n",
    "\n",
    "\n",
    "data_files = os.listdir('./results/modelling/2025-06-11') + os.listdir('./results/modelling/2025-06-12')\n",
    "\n",
    "# Define the two directories\n",
    "dir1 = Path('./results/modelling/2025-06-11')\n",
    "dir2 = Path('./results/modelling/2025-06-12')\n",
    "\n",
    "# Get all files recursively in both directories\n",
    "files = list(dir1.rglob('*')) + list(dir2.rglob('*'))\n",
    "\n",
    "# Filter only files (exclude directories) and convert to absolute paths\n",
    "file_paths = [f.resolve() for f in files if f.is_file()]\n",
    "\n",
    "all_dataset_metrics = []\n",
    "# Print or use the paths\n",
    "for path in file_paths:\n",
    "    print(path)\n",
    "    pattern = re.compile(r\"all_metrics_(.*?)_LOOCV\")\n",
    "    match = pattern.search(path.stem)\n",
    "    dataset = match.group(1)\n",
    "    print(dataset)\n",
    "\n",
    "    all_metrics_df = pd.read_csv(path)\n",
    "    model_dfs = [calculate_metrics_leave_one_out(all_metrics_df, model_name)\n",
    "                for model_name in all_metrics_df['model'].unique()]\n",
    "\n",
    "    metrics_global = pd.concat(model_dfs, ignore_index=True)\n",
    "    metrics_global['dataset'] = dataset\n",
    "    metrics_global['PCA'] = True if 'PCA' in str(path) else False\n",
    "    \n",
    "\n",
    "    all_dataset_metrics.append(metrics_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f4f0587f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>auc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>dataset</th>\n",
       "      <th>PCA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.857692</td>\n",
       "      <td>0.660714</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.716418</td>\n",
       "      <td>non_digital_test_less_subjects</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.814103</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.767949</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>non_digital_test_less_subjects</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.764479</td>\n",
       "      <td>0.721519</td>\n",
       "      <td>0.723616</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>non_digital_tests</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.721519</td>\n",
       "      <td>0.723616</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>non_digital_tests</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.667310</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.630631</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.658824</td>\n",
       "      <td>demographic+digital</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.607595</td>\n",
       "      <td>0.605212</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>digital_test</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.627413</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.674157</td>\n",
       "      <td>demographic</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.601282</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.555128</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.561404</td>\n",
       "      <td>digital_test_less_subjects</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.560489</td>\n",
       "      <td>0.531646</td>\n",
       "      <td>0.527349</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.574713</td>\n",
       "      <td>demographic+digital</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.559202</td>\n",
       "      <td>0.531646</td>\n",
       "      <td>0.527349</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.574713</td>\n",
       "      <td>digital_test</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.557272</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.501931</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>demographic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.537179</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>hand_and_eye</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.494872</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.508974</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>hand_and_eye</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.508974</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>digital_test_less_subjects</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model       auc  accuracy  balanced_accuracy  precision  \\\n",
       "1                 SVC  0.857692  0.660714           0.650000   0.648649   \n",
       "2  LogisticRegression  0.814103  0.767857           0.767949   0.793103   \n",
       "2  LogisticRegression  0.764479  0.721519           0.723616   0.763158   \n",
       "2  LogisticRegression  0.756757  0.721519           0.723616   0.763158   \n",
       "2  LogisticRegression  0.667310  0.632911           0.630631   0.651163   \n",
       "2  LogisticRegression  0.648649  0.607595           0.605212   0.627907   \n",
       "3       XGBClassifier  0.612613  0.632911           0.627413   0.638298   \n",
       "2  LogisticRegression  0.601282  0.553571           0.555128   0.592593   \n",
       "2  LogisticRegression  0.560489  0.531646           0.527349   0.555556   \n",
       "2  LogisticRegression  0.559202  0.531646           0.527349   0.555556   \n",
       "3       XGBClassifier  0.557272  0.506329           0.501931   0.533333   \n",
       "2  LogisticRegression  0.537179  0.517857           0.516667   0.551724   \n",
       "2  LogisticRegression  0.494872  0.517857           0.508974   0.542857   \n",
       "2  LogisticRegression  0.450000  0.517857           0.508974   0.542857   \n",
       "\n",
       "     recall        f1                         dataset    PCA  \n",
       "1  0.800000  0.716418  non_digital_test_less_subjects   True  \n",
       "2  0.766667  0.779661  non_digital_test_less_subjects  False  \n",
       "2  0.690476  0.725000               non_digital_tests   True  \n",
       "2  0.690476  0.725000               non_digital_tests  False  \n",
       "2  0.666667  0.658824             demographic+digital  False  \n",
       "2  0.642857  0.635294                    digital_test  False  \n",
       "3  0.714286  0.674157                     demographic  False  \n",
       "2  0.533333  0.561404      digital_test_less_subjects  False  \n",
       "2  0.595238  0.574713             demographic+digital   True  \n",
       "2  0.595238  0.574713                    digital_test   True  \n",
       "3  0.571429  0.551724                     demographic   True  \n",
       "2  0.533333  0.542373                    hand_and_eye  False  \n",
       "2  0.633333  0.584615                    hand_and_eye   True  \n",
       "2  0.633333  0.584615      digital_test_less_subjects   True  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datasets_df = pd.concat(all_dataset_metrics)\n",
    "all_datasets_df.groupby(['model', 'dataset', 'PCA'])\n",
    "top1_per_dataset = all_datasets_df.groupby(['dataset', 'PCA'], group_keys=False).apply(\n",
    "    lambda group: group.nlargest(1, 'auc')\n",
    ")\n",
    "top1_per_dataset.sort_values('auc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "723e2d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>auc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>dataset</th>\n",
       "      <th>PCA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.667310</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.630631</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.658824</td>\n",
       "      <td>demographic+digital</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.607595</td>\n",
       "      <td>0.605212</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>digital_test</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.640927</td>\n",
       "      <td>0.594937</td>\n",
       "      <td>0.591699</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>digital_test</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.627413</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.674157</td>\n",
       "      <td>demographic</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.601282</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.555128</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.561404</td>\n",
       "      <td>digital_test_less_subjects</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.594937</td>\n",
       "      <td>0.593308</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>demographic+digital</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.583012</td>\n",
       "      <td>0.569620</td>\n",
       "      <td>0.566281</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>digital_test</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.579794</td>\n",
       "      <td>0.607595</td>\n",
       "      <td>0.605212</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>demographic+digital</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.572716</td>\n",
       "      <td>0.493671</td>\n",
       "      <td>0.491634</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>demographic</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.569498</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.503539</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>digital_test</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.561776</td>\n",
       "      <td>0.518987</td>\n",
       "      <td>0.517053</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>demographic+digital</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.560489</td>\n",
       "      <td>0.531646</td>\n",
       "      <td>0.527349</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.574713</td>\n",
       "      <td>demographic+digital</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.559202</td>\n",
       "      <td>0.531646</td>\n",
       "      <td>0.527349</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.574713</td>\n",
       "      <td>digital_test</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.557272</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.501931</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>demographic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.556410</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.494872</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>digital_test_less_subjects</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.543115</td>\n",
       "      <td>0.518987</td>\n",
       "      <td>0.517053</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>demographic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.537179</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>hand_and_eye</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.511261</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.501931</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>digital_test</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.508366</td>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.571750</td>\n",
       "      <td>0.584906</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.652632</td>\n",
       "      <td>demographic</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.508366</td>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.571750</td>\n",
       "      <td>0.584906</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.652632</td>\n",
       "      <td>demographic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.501287</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.501931</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>demographic+digital</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.494872</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.508974</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>hand_and_eye</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.480695</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.625804</td>\n",
       "      <td>0.632653</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.681319</td>\n",
       "      <td>demographic</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.480695</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.625804</td>\n",
       "      <td>0.632653</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.681319</td>\n",
       "      <td>demographic</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.474903</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.498713</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>digital_test</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.471042</td>\n",
       "      <td>0.493671</td>\n",
       "      <td>0.488417</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>demographic+digital</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.508974</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>digital_test_less_subjects</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.444231</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>digital_test_less_subjects</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.432692</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.451282</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>hand_and_eye</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.429487</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.514103</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.557377</td>\n",
       "      <td>digital_test_less_subjects</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.412821</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.453846</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>digital_test_less_subjects</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.403846</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>digital_test_less_subjects</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.381410</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.417949</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>hand_and_eye</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.492308</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>hand_and_eye</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.355128</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.437179</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.523077</td>\n",
       "      <td>hand_and_eye</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.330759</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.497104</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>demographic+digital</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.320513</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.403846</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>digital_test_less_subjects</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.317949</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.425641</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>hand_and_eye</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.302564</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.489744</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>hand_and_eye</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.251287</td>\n",
       "      <td>0.531646</td>\n",
       "      <td>0.525740</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.584270</td>\n",
       "      <td>digital_test</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model       auc  accuracy  balanced_accuracy  precision  \\\n",
       "2      LogisticRegression  0.667310  0.632911           0.630631   0.651163   \n",
       "2      LogisticRegression  0.648649  0.607595           0.605212   0.627907   \n",
       "1                     SVC  0.640927  0.594937           0.591699   0.613636   \n",
       "3           XGBClassifier  0.612613  0.632911           0.627413   0.638298   \n",
       "2      LogisticRegression  0.601282  0.553571           0.555128   0.592593   \n",
       "1                     SVC  0.595238  0.594937           0.593308   0.619048   \n",
       "3           XGBClassifier  0.583012  0.569620           0.566281   0.590909   \n",
       "3           XGBClassifier  0.579794  0.607595           0.605212   0.627907   \n",
       "0  RandomForestClassifier  0.572716  0.493671           0.491634   0.523810   \n",
       "0  RandomForestClassifier  0.569498  0.506329           0.503539   0.534884   \n",
       "0  RandomForestClassifier  0.561776  0.518987           0.517053   0.547619   \n",
       "2      LogisticRegression  0.560489  0.531646           0.527349   0.555556   \n",
       "2      LogisticRegression  0.559202  0.531646           0.527349   0.555556   \n",
       "3           XGBClassifier  0.557272  0.506329           0.501931   0.533333   \n",
       "1                     SVC  0.556410  0.500000           0.494872   0.531250   \n",
       "0  RandomForestClassifier  0.543115  0.518987           0.517053   0.547619   \n",
       "2      LogisticRegression  0.537179  0.517857           0.516667   0.551724   \n",
       "0  RandomForestClassifier  0.511261  0.506329           0.501931   0.533333   \n",
       "2      LogisticRegression  0.508366  0.582278           0.571750   0.584906   \n",
       "2      LogisticRegression  0.508366  0.582278           0.571750   0.584906   \n",
       "3           XGBClassifier  0.501287  0.506329           0.501931   0.533333   \n",
       "2      LogisticRegression  0.494872  0.517857           0.508974   0.542857   \n",
       "1                     SVC  0.480695  0.632911           0.625804   0.632653   \n",
       "1                     SVC  0.480695  0.632911           0.625804   0.632653   \n",
       "3           XGBClassifier  0.474903  0.506329           0.498713   0.530612   \n",
       "0  RandomForestClassifier  0.471042  0.493671           0.488417   0.521739   \n",
       "2      LogisticRegression  0.450000  0.517857           0.508974   0.542857   \n",
       "0  RandomForestClassifier  0.444231  0.392857           0.384615   0.441176   \n",
       "0  RandomForestClassifier  0.432692  0.464286           0.451282   0.500000   \n",
       "3           XGBClassifier  0.429487  0.517857           0.514103   0.548387   \n",
       "1                     SVC  0.412821  0.464286           0.453846   0.500000   \n",
       "3           XGBClassifier  0.403846  0.428571           0.423077   0.468750   \n",
       "0  RandomForestClassifier  0.381410  0.428571           0.417949   0.472222   \n",
       "1                     SVC  0.366667  0.500000           0.492308   0.529412   \n",
       "3           XGBClassifier  0.355128  0.446429           0.437179   0.485714   \n",
       "1                     SVC  0.330759  0.506329           0.497104   0.529412   \n",
       "0  RandomForestClassifier  0.320513  0.410714           0.403846   0.454545   \n",
       "3           XGBClassifier  0.317949  0.428571           0.425641   0.466667   \n",
       "1                     SVC  0.302564  0.500000           0.489744   0.527778   \n",
       "1                     SVC  0.251287  0.531646           0.525740   0.553191   \n",
       "\n",
       "     recall        f1                     dataset    PCA  \n",
       "2  0.666667  0.658824         demographic+digital  False  \n",
       "2  0.642857  0.635294                digital_test  False  \n",
       "1  0.642857  0.627907                digital_test  False  \n",
       "3  0.714286  0.674157                 demographic  False  \n",
       "2  0.533333  0.561404  digital_test_less_subjects  False  \n",
       "1  0.619048  0.619048         demographic+digital  False  \n",
       "3  0.619048  0.604651                digital_test  False  \n",
       "3  0.642857  0.635294         demographic+digital  False  \n",
       "0  0.523810  0.523810                 demographic  False  \n",
       "0  0.547619  0.541176                digital_test  False  \n",
       "0  0.547619  0.547619         demographic+digital  False  \n",
       "2  0.595238  0.574713         demographic+digital   True  \n",
       "2  0.595238  0.574713                digital_test   True  \n",
       "3  0.571429  0.551724                 demographic   True  \n",
       "1  0.566667  0.548387  digital_test_less_subjects  False  \n",
       "0  0.547619  0.547619                 demographic   True  \n",
       "2  0.533333  0.542373                hand_and_eye  False  \n",
       "0  0.571429  0.551724                digital_test   True  \n",
       "2  0.738095  0.652632                 demographic  False  \n",
       "2  0.738095  0.652632                 demographic   True  \n",
       "3  0.571429  0.551724         demographic+digital   True  \n",
       "2  0.633333  0.584615                hand_and_eye   True  \n",
       "1  0.738095  0.681319                 demographic  False  \n",
       "1  0.738095  0.681319                 demographic   True  \n",
       "3  0.619048  0.571429                digital_test   True  \n",
       "0  0.571429  0.545455         demographic+digital   True  \n",
       "2  0.633333  0.584615  digital_test_less_subjects   True  \n",
       "0  0.500000  0.468750  digital_test_less_subjects  False  \n",
       "0  0.633333  0.558824                hand_and_eye  False  \n",
       "3  0.566667  0.557377  digital_test_less_subjects  False  \n",
       "1  0.600000  0.545455  digital_test_less_subjects   True  \n",
       "3  0.500000  0.483871  digital_test_less_subjects   True  \n",
       "0  0.566667  0.515152                hand_and_eye   True  \n",
       "1  0.600000  0.562500                hand_and_eye  False  \n",
       "3  0.566667  0.523077                hand_and_eye   True  \n",
       "1  0.642857  0.580645         demographic+digital   True  \n",
       "0  0.500000  0.476190  digital_test_less_subjects   True  \n",
       "3  0.466667  0.466667                hand_and_eye  False  \n",
       "1  0.633333  0.575758                hand_and_eye   True  \n",
       "1  0.619048  0.584270                digital_test   True  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datasets_df.sort_values('auc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1dbcaab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "def plot_model_metrics(path):\n",
    "    all_metrics_df = pd.read_csv(path)\n",
    "\n",
    "    # Extraer nombre del dataset\n",
    "    match = re.search(r\"(?<=all_metrics_).*(?=_nested4x10\\.csv)\", path)\n",
    "    dataset = match.group() if match else \"unknown\"\n",
    "    print(\"Dataset name:\", dataset)\n",
    "\n",
    "    # Tabla de medias por modelo\n",
    "    metrics_comparison = all_metrics_df.groupby('model').mean(numeric_only=True)\n",
    "    # display(bold_max(metrics_comparison, dataset=dataset))\n",
    "\n",
    "    # Crear tabla con formato \"mean [min max]\" por modelo y métrica\n",
    "    metrics_to_plot = ['balanced_accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "    summary_formatted = []\n",
    "\n",
    "    for model in all_metrics_df['model'].unique():\n",
    "        row = {'model': model}\n",
    "        df_model = all_metrics_df[all_metrics_df['model'] == model]\n",
    "        for metric in metrics_to_plot:\n",
    "            mean_val = df_model[metric].mean()\n",
    "            min_val = df_model[metric].min()\n",
    "            max_val = df_model[metric].max()\n",
    "            row[metric] = f\"{mean_val:.3f} [{min_val:.3f} {max_val:.3f}]\"\n",
    "        summary_formatted.append(row)\n",
    "\n",
    "    formatted_df = pd.DataFrame(summary_formatted)\n",
    "    print(\"\\nResumen de métricas por modelo (mean [min max]):\")\n",
    "    display(formatted_df)\n",
    "\n",
    "    # Boxplot\n",
    "    df_long = all_metrics_df.melt(\n",
    "        id_vars=['model', 'repeat', 'fold'],\n",
    "        value_vars=metrics_to_plot,\n",
    "        var_name='metric',\n",
    "        value_name='score'\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.boxplot(\n",
    "        data=df_long,\n",
    "        x='metric',\n",
    "        y='score',\n",
    "        hue='model',\n",
    "        linewidth=1.5\n",
    "    )\n",
    "\n",
    "    formatted_df['dataset'] = dataset\n",
    "    plt.axhline(0.5, color='gray', linestyle='--', linewidth=1.4)\n",
    "    plt.title(f'Distribución de métricas para el dataset \"{dataset}\"', fontsize=16)\n",
    "    plt.xlabel('Métrica', fontsize=14)\n",
    "    plt.ylabel('Score', fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(title='Modelo', fontsize=10, title_fontsize=8, loc='best', bbox_to_anchor=(0, 0, 1, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return formatted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b1967",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_results = []\n",
    "for dataset in ['pca', 'demographic', 'digital_test', 'demographic+digital', 'hand_and_eye', 'non_digital_tests', 'digital_test_less_subjects', 'hand_and_eye_demo']:\n",
    "    df_res = plot_model_metrics(f'/home/gus/Documents/REPOS/tmt-analysis/notebooks/results/modelling/resultados_seminario_06_06_25/all_metrics_{dataset}_nested4x10.csv')\n",
    "    dfs_results.append(df_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feedback",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
